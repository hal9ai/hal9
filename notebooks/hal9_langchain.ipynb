{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "Ue34YR1QHucf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "langchain-openai\n",
        "langchain-community"
      ],
      "metadata": {
        "id": "GJO4cKc6_2Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "MR9gRLKR_6d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlqdYuiB5Qp7"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# File to persist chat history\n",
        "os.makedirs(\".storage\", exist_ok=True)\n",
        "HISTORY_FILE = \".storage/.history.pkl\"\n",
        "\n",
        "# Load history from file or create new\n",
        "if os.path.exists(HISTORY_FILE):\n",
        "    with open(HISTORY_FILE, \"rb\") as f:\n",
        "        history = pickle.load(f)\n",
        "else:\n",
        "    history = ChatMessageHistory()\n",
        "\n",
        "# Initialize OpenAI model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7,\n",
        "    openai_api_key=\"sk-proj-xxx\"\n",
        ")\n",
        "\n",
        "# Prompt template with message history\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer concisely and accurately.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create chat chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# Set up chatbot with history\n",
        "chatbot = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda _: history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# Get user input and respond\n",
        "user_input = input()\n",
        "response = chatbot.invoke({\"input\": user_input}, config={\"configurable\": {\"session_id\": \"ignored\"}})\n",
        "print(response.content)\n",
        "\n",
        "# Save updated history\n",
        "with open(HISTORY_FILE, \"wb\") as f:\n",
        "    pickle.dump(history, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XG8Ls3lV_Y-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "YDyXT17RG5l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hal9"
      ],
      "metadata": {
        "id": "JeOUygRH_jWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!HAL9_TOKEN=H9XXX hal9 deploy . --name langchain-demo"
      ],
      "metadata": {
        "id": "MHceavHvARn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}